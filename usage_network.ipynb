{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network(QNet) 사용 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import 및 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from network import QTrainer, QNet\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "SEED = 1                     # A seed for the random number generator\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Graph\n",
    "NR_NODES = 998               # Number of nodes N\n",
    "EMBEDDING_DIMENSIONS = 5     # Embedding dimension D\n",
    "EMBEDDING_ITERATIONS_T = 1   # Number of embedding iterations T\n",
    "\n",
    "# Learning\n",
    "NR_EPISODES = 100\n",
    "MEMORY_CAPACITY = 10000\n",
    "N_STEP_QL = 2                # Number of steps (n) in n-step Q-learning to wait before computing target reward estimate\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "GAMMA = 0.9\n",
    "INIT_LR = 5e-3\n",
    "LR_DECAY_RATE = 1. - 2e-5    # learning rate decay\n",
    "\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_DECAY_RATE = 6e-4    # epsilon decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State, action 관련 자료형, 함수, 클래스 정의\n",
    "- State : 현재 state에 대한 정보를 저장하기 위한 자료형 \n",
    "- Experience : state tensor들을 한번만 계산하기 위해 experience 인스턴스에 저장합니다.  \n",
    "- state2tens : state를 5개의 차원으로 embedding하는 함수  \n",
    "- 여러 experience를 저장해두기 위한 메모리 클래스  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ('W', 'coords', 'partial_solution'))\n",
    "Experience = namedtuple('Experience', ('state', 'state_tsr', 'action', 'reward', 'next_state', 'next_state_tsr'))\n",
    "\n",
    "\n",
    "def state2tens(state):\n",
    "    solution = set(state.partial_solution)\n",
    "    sol_last_node = state.partial_solution[-1] if len(state.partial_solution) > 0 else -1\n",
    "    sol_first_node = state.partial_solution[0] if len(state.partial_solution) > 0 else -1\n",
    "    coords = state.coords\n",
    "    nr_nodes = coords.shape[0]\n",
    "\n",
    "    xv = [[(1 if i in solution else 0),           # 해당 노드를 방문 했는지 여부\n",
    "           (1 if i == sol_first_node else 0),     # 해당 노드가 시작 노드인지 여부 \n",
    "           (1 if i == sol_last_node else 0),      # 해당 노드가 마지막 노드인지 여부\n",
    "           coords[i,0],                           # 해당 노드의 x좌표\n",
    "           coords[i,1]                            # 해당 노드의 y좌표\n",
    "          ] for i in range(nr_nodes)]\n",
    "    \n",
    "    return torch.tensor(xv, dtype=torch.float32, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.nr_inserts = 0\n",
    "        \n",
    "    def remember(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.nr_inserts += 1\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.nr_inserts, self.capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 외의 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_distance(solution, W):\n",
    "    if len(solution) < 2:\n",
    "        return 0 \n",
    "    \n",
    "    total_dist = 0\n",
    "    for i in range(len(solution) - 1):\n",
    "        total_dist += W[solution[i], solution[i+1]].item()\n",
    "        \n",
    "    if len(solution) == W.shape[0]:\n",
    "        total_dist += W[solution[-1], solution[0]].item()\n",
    "\n",
    "    return total_dist\n",
    "\n",
    "        \n",
    "def is_state_final(state):\n",
    "    return len(set(state.partial_solution)) == state.W.shape[0]\n",
    "\n",
    "\n",
    "def get_next_neighbor_random(state):\n",
    "    solution, W = state.partial_solution, state.W\n",
    "    \n",
    "    if len(solution) == 0:\n",
    "        return random.choice(range(W.shape[0]))\n",
    "    already_in = set(solution)\n",
    "    candidates = list(filter(lambda n: n.item() not in already_in, W[solution[-1]].nonzero()))\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    return random.choice(candidates).item()\n",
    "\n",
    "\n",
    "def get_distance_matrix(x, num_cities=998):\n",
    "    x = torch.tensor(x)\n",
    "    x1, x2 = x[:,0:1], x[:,1:2]\n",
    "    d1 = x1 - (x1.T).repeat(num_cities,1)\n",
    "    d2 = x2 - (x2.T).repeat(num_cities,1)\n",
    "    distance_matrix = (d1**2 + d2**2)**0.5   # Euclidean Distance\n",
    "    return distance_matrix.numpy()\n",
    "\n",
    "\n",
    "def init_model(fname=None):\n",
    "    Q_net = QNet(EMBEDDING_DIMENSIONS, T=EMBEDDING_ITERATIONS_T).to(device)\n",
    "    optimizer = optim.Adam(Q_net.parameters(), lr=INIT_LR)\n",
    "    lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY_RATE)\n",
    "    \n",
    "    if fname is not None:\n",
    "        checkpoint = torch.load(fname)\n",
    "        Q_net.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    \n",
    "    Q_trainer = QTrainer(Q_net, optimizer, lr_scheduler)\n",
    "    return Q_trainer, Q_net, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSP Data Load\n",
    "coords = np.array(pd.read_csv('2024_AI_TSP.csv', header=None))\n",
    "\n",
    "# make distance matrix\n",
    "W_np = get_distance_matrix(coords)\n",
    "\n",
    "# init Trainer, Model\n",
    "Q_trainer, Q_net, optimizer, lr_scheduler = init_model()\n",
    "\n",
    "# generate memory\n",
    "memory = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "\n",
    "losses = []\n",
    "path_lengths = []\n",
    "found_solutions = dict()\n",
    "current_min_med_length = float('inf')\n",
    "\n",
    "\n",
    "for episode in range(NR_EPISODES):\n",
    "    \n",
    "    # tensor (distance matrix)\n",
    "    W = torch.tensor(W_np, dtype=torch.float32, requires_grad=False, device=device)\n",
    "    \n",
    "    # start node = 0\n",
    "    solution = [0]\n",
    "    \n",
    "    # current state\n",
    "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "    current_state_tsr = state2tens(current_state)\n",
    "    \n",
    "\n",
    "    # define state, state_tsrs(embedding), reward, action list\n",
    "    states = [current_state]\n",
    "    states_tsrs = [current_state_tsr] \n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    \n",
    "    # current value of epsilon\n",
    "    epsilon = max(MIN_EPSILON, (1-EPSILON_DECAY_RATE)**episode)\n",
    "    \n",
    "\n",
    "    while not is_state_final(current_state):\n",
    "        \n",
    "        # select next node\n",
    "        if epsilon >= random.random():\n",
    "            next_node = get_next_neighbor_random(current_state)\n",
    "        else:\n",
    "            next_node, est_reward = Q_trainer.get_best_action(current_state_tsr, current_state)\n",
    "        \n",
    "\n",
    "        # append next node to solution\n",
    "        next_solution = solution + [next_node]\n",
    "\n",
    "        # calulate reward\n",
    "        reward = -(total_distance(next_solution, W) - total_distance(solution, W))\n",
    "        \n",
    "        \n",
    "        next_state = State(partial_solution=next_solution, W=W, coords=coords)\n",
    "        next_state_tsr = state2tens(next_state)\n",
    "        \n",
    "        states.append(next_state)\n",
    "        states_tsrs.append(next_state_tsr)\n",
    "        rewards.append(reward)\n",
    "        actions.append(next_node)\n",
    "        \n",
    "        \n",
    "        if len(solution) >= N_STEP_QL:\n",
    "            memory.remember(Experience(state=states[-N_STEP_QL],\n",
    "                                       state_tsr=states_tsrs[-N_STEP_QL],\n",
    "                                       action=actions[-N_STEP_QL],\n",
    "                                       reward=sum(rewards[-N_STEP_QL:]),\n",
    "                                       next_state=next_state,\n",
    "                                       next_state_tsr=next_state_tsr))\n",
    "            \n",
    "        if is_state_final(next_state):\n",
    "            for n in range(1, N_STEP_QL):\n",
    "                memory.remember(Experience(state=states[-n],\n",
    "                                           state_tsr=states_tsrs[-n], \n",
    "                                           action=actions[-n], \n",
    "                                           reward=sum(rewards[-n:]), \n",
    "                                           next_state=next_state,\n",
    "                                           next_state_tsr=next_state_tsr))\n",
    "        \n",
    "        \n",
    "        current_state = next_state\n",
    "        current_state_tsr = next_state_tsr\n",
    "        solution = next_solution\n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "\n",
    "            # sampling batch experience\n",
    "            experiences = memory.sample_batch(BATCH_SIZE)\n",
    "            \n",
    "            batch_states_tsrs = [e.state_tsr for e in experiences]\n",
    "            batch_Ws = [e.state.W for e in experiences]\n",
    "            batch_actions = [e.action for e in experiences]\n",
    "            batch_targets = []\n",
    "            \n",
    "\n",
    "            for i, experience in enumerate(experiences):\n",
    "                target = experience.reward\n",
    "                if not is_state_final(experience.next_state):\n",
    "                    _, best_q_value = Q_trainer.get_best_action(experience.next_state_tsr, experience.next_state)\n",
    "                    target += GAMMA * best_q_value\n",
    "                batch_targets.append(target)\n",
    "                \n",
    "            loss = Q_trainer.batch_update(batch_states_tsrs, batch_Ws, batch_actions, batch_targets)\n",
    "            losses.append(loss)\n",
    "\n",
    "    length = total_distance(solution, W)\n",
    "    path_lengths.append(length)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print('Ep %d. Loss = %.3f, length = %.3f, epsilon = %.4f, lr = %.4f' % (\n",
    "            episode, (-1 if loss is None else loss), length, epsilon,\n",
    "            Q_trainer.optimizer.param_groups[0]['lr']))\n",
    "        found_solutions[episode] = (W.clone(), coords.copy(), [n for n in solution])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [0]\n",
    "current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "current_state_tsr = state2tens(current_state)\n",
    "\n",
    "while not is_state_final(current_state):\n",
    "    next_node, est_reward = Q_trainer.get_best_action(current_state_tsr, \n",
    "                                                    current_state)\n",
    "    \n",
    "    solution = solution + [next_node]\n",
    "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "    current_state_tsr = state2tens(current_state)\n",
    "    \n",
    "print(\"Final solution : \", str(solution))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
